# Table-of-Contents

<!-- toc -->

- [üß† **MICRO-NOTES ‚Äî API Calls, Memory & Tool Calling**](#%F0%9F%A7%A0-micro-notes--api-calls-memory--tool-calling)
  * [**1Ô∏è‚É£ Core Idea**](#1%EF%B8%8F%E2%83%A3-core-idea)
  * [**2Ô∏è‚É£ LLM API Call Lifecycle**](#2%EF%B8%8F%E2%83%A3-llm-api-call-lifecycle)
  * [**3Ô∏è‚É£ Memory Types**](#3%EF%B8%8F%E2%83%A3-memory-types)
    + [**Short-Term Memory**](#short-term-memory)
    + [**Long-Term Memory**](#long-term-memory)
    + [**System / Profile Memory**](#system--profile-memory)
  * [**4Ô∏è‚É£ Context Window & Trade-offs**](#4%EF%B8%8F%E2%83%A3-context-window--trade-offs)
  * [**5Ô∏è‚É£ What Tool Calling Really Is**](#5%EF%B8%8F%E2%83%A3-what-tool-calling-really-is)
  * [**6Ô∏è‚É£ Tool Selection & Orchestration**](#6%EF%B8%8F%E2%83%A3-tool-selection--orchestration)
  * [**7Ô∏è‚É£ Production Architecture Pattern**](#7%EF%B8%8F%E2%83%A3-production-architecture-pattern)
  * [**8Ô∏è‚É£ Failure Modes (Interview Gold)**](#8%EF%B8%8F%E2%83%A3-failure-modes-interview-gold)
  * [**9Ô∏è‚É£ One-Minute Interview Explanation**](#9%EF%B8%8F%E2%83%A3-one-minute-interview-explanation)

<!-- tocstop -->

---

# üß† **MICRO-NOTES ‚Äî API Calls, Memory & Tool Calling**

---

## **1Ô∏è‚É£ Core Idea**

> **LLMs are stateless engines.**  
> Your backend provides **memory, tools, and control.**

The model only sees:

```
current prompt ‚Üí response
```

Everything else is **your system‚Äôs job.**

---

## **2Ô∏è‚É£ LLM API Call Lifecycle**

**Mental Diagram**

```
User Input
 ‚Üí Backend
   ‚Üí Inject system prompt
   ‚Üí Inject memory
   ‚Üí Inject tool schemas
 ‚Üí LLM API Call
 ‚Üí LLM decides: text or tool
 ‚Üí Backend executes tools
 ‚Üí Results appended to context
 ‚Üí LLM produces final answer
```

**Interview line:**

> ‚ÄúThe LLM is just a reasoning engine; orchestration lives in the backend.‚Äù

---

## **3Ô∏è‚É£ Memory Types**

### **Short-Term Memory**

- Chat history in context window
    
- Limited by token budget
    

### **Long-Term Memory**

- Stored externally (DB / vector store)
    
- Retrieved and injected when relevant
    

### **System / Profile Memory**

- User preferences
    
- App configuration
    
- Long-term persona info
    

**Key Point:**

> ‚ÄúMemory is not inside the model ‚Äî it‚Äôs reconstructed every request.‚Äù

---

## **4Ô∏è‚É£ Context Window & Trade-offs**

LLM only knows what‚Äôs inside the prompt.

Trade-offs:

- More context ‚Üí better grounding
    
- More context ‚Üí higher cost & slower
    
- Too much context ‚Üí performance drops
    

**Production rule:**

> Always send **only the most relevant memory.**

---

## **5Ô∏è‚É£ What Tool Calling Really Is**

LLMs **cannot call APIs**.

Instead:

- You provide **tool schemas** (name, inputs, description)
    
- The LLM outputs:
    
    ```
    { tool_name, arguments }
    ```
    
- Backend executes the tool
    
- Tool result is fed back into the LLM
    

**Interview line:**

> ‚ÄúLLMs suggest actions; the backend executes them.‚Äù

---

## **6Ô∏è‚É£ Tool Selection & Orchestration**

Model chooses tools using:

- Tool descriptions
    
- Current prompt
    
- Conversation context
    

Flow:

```
LLM ‚Üí Tool decision ‚Üí Backend runs tool ‚Üí Result ‚Üí LLM ‚Üí Final answer
```

Multiple tools can be chained.

---

## **7Ô∏è‚É£ Production Architecture Pattern**

```
User
 ‚Üí Backend
   ‚Üí Memory retrieval
   ‚Üí Prompt construction
   ‚Üí Tool schemas
 ‚Üí LLM
   ‚Üí Tool request
 ‚Üí Backend executes tools
 ‚Üí LLM final response
 ‚Üí User
```

---

## **8Ô∏è‚É£ Failure Modes (Interview Gold)**

- Hallucinated tool calls
    
- Wrong tool selection
    
- Context window overflow
    
- Memory leakage between users
    
- Cost explosion from oversized prompts
    

**Engineering fix:**  
Strict schemas, validation, pruning memory, evals.

---

## **9Ô∏è‚É£ One-Minute Interview Explanation**

> ‚ÄúLLMs are stateless. Each API call is a fresh request where we inject memory, instructions, and tool definitions.  
> The model reasons over this context and may request a tool.  
> The backend executes it, returns results, and the LLM produces the final answer.  
> Real AI systems are mostly orchestration, not modeling.‚Äù

---
