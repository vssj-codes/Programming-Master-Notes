# Table-of-Contents

<!-- toc -->

- [üß† **MICRO-NOTES ‚Äî Sampling, SFT, RLHF**](#%F0%9F%A7%A0-micro-notes--sampling-sft-rlhf)
  * [**1Ô∏è‚É£ One-Line Core Idea**](#1%EF%B8%8F%E2%83%A3-one-line-core-idea)
  * [**2Ô∏è‚É£ Autoregressive Language Modeling (GPT-Style)**](#2%EF%B8%8F%E2%83%A3-autoregressive-language-modeling-gpt-style)
    + [**What GPT-2 / GPT-3 / GPT-4 are**](#what-gpt-2--gpt-3--gpt-4-are)
    + [**Training Objective**](#training-objective)
    + [**Inference Loop (Mental Flow)**](#inference-loop-mental-flow)
  * [**3Ô∏è‚É£ Logits & Sampling**](#3%EF%B8%8F%E2%83%A3-logits--sampling)
    + [**Logits**](#logits)
    + [**Sampling Strategies**](#sampling-strategies)
  * [**4Ô∏è‚É£ Attention Mechanism (Core of Transformers)**](#4%EF%B8%8F%E2%83%A3-attention-mechanism-core-of-transformers)
    + [**QKV Intuition**](#qkv-intuition)
    + [**Scaled Dot-Product Attention**](#scaled-dot-product-attention)
    + [**Multi-Head Attention**](#multi-head-attention)
    + [**Causal Masking (GPT)**](#causal-masking-gpt)
  * [**5Ô∏è‚É£ SFT & RLHF (Why ChatGPT behaves nicely)**](#5%EF%B8%8F%E2%83%A3-sft--rlhf-why-chatgpt-behaves-nicely)
    + [**Supervised Fine-Tuning (SFT)**](#supervised-fine-tuning-sft)
    + [**RLHF (Reinforcement Learning from Human Feedback)**](#rlhf-reinforcement-learning-from-human-feedback)
  * [**6Ô∏è‚É£ Tokenization Tooling**](#6%EF%B8%8F%E2%83%A3-tokenization-tooling)
  * [**7Ô∏è‚É£ Failure & Trade-Off Points**](#7%EF%B8%8F%E2%83%A3-failure--trade-off-points)
  * [**8Ô∏è‚É£ One-Minute Interview Explanation**](#8%EF%B8%8F%E2%83%A3-one-minute-interview-explanation)

<!-- tocstop -->

---

# üß† **MICRO-NOTES ‚Äî Sampling, SFT, RLHF**

---

## **1Ô∏è‚É£ One-Line Core Idea**

> _LLMs generate text by predicting the next token autoregressively using probabilities, and their behavior is shaped using fine-tuning (SFT) and human feedback (RLHF)._

---

## **2Ô∏è‚É£ Autoregressive Language Modeling (GPT-Style)**

### **What GPT-2 / GPT-3 / GPT-4 are**

- **Decoder-only transformers**
    
- Trained to predict:  
    `P(next_token | all_previous_tokens)`
    

### **Training Objective**

For sequence:  
`x1, x2, ‚Ä¶, xT`

Model maximizes:

> **Œ£ log P(xt | x1‚Ä¶x(t-1))**

### **Inference Loop (Mental Flow)**

```
Prompt
 ‚Üí Tokenize
 ‚Üí Forward pass
 ‚Üí Logits
 ‚Üí Softmax ‚Üí Probabilities
 ‚Üí Sampling strategy
 ‚Üí Next token
 ‚Üí Append ‚Üí Repeat
```

**Interview Line:**

> ‚ÄúLLMs don‚Äôt generate sentences ‚Äî they generate one token at a time using conditional probabilities.‚Äù

---

## **3Ô∏è‚É£ Logits & Sampling**

### **Logits**

- Raw scores from the model.
    
- Converted to probabilities via **softmax**.
    

### **Sampling Strategies**

|Strategy|What it does|Trade-off|
|---|---|---|
|Greedy|Pick highest prob token|Deterministic, boring|
|Temperature|Controls randomness|High T = creative, low T = safe|
|Top-K|Sample from K most likely tokens|Limits tail risk|
|Top-P (Nucleus)|Sample from smallest set whose probs sum to P|More adaptive|
|Beam Search|Keep multiple best sequences|Better for structured output|

**Interview Line:**

> ‚ÄúSampling controls creativity vs reliability ‚Äî temperature and top-p are the main production knobs.‚Äù

---

## **4Ô∏è‚É£ Attention Mechanism (Core of Transformers)**

### **QKV Intuition**

For each token:

- **Query (Q)** ‚Üí what am I looking for?
    
- **Key (K)** ‚Üí what do I represent?
    
- **Value (V)** ‚Üí what info do I carry?
    

### **Scaled Dot-Product Attention**

```
Attention(Q,K,V) = softmax(QK·µÄ / ‚àöd) ¬∑ V
```

### **Multi-Head Attention**

- Multiple heads learn different relationships.
    
- Outputs concatenated ‚Üí projected back to hidden size.
    

### **Causal Masking (GPT)**

- Token can attend **only to past tokens**
    
- Prevents future leakage during generation.
    

**Interview Line:**

> ‚ÄúSelf-attention lets each token decide which previous tokens matter most.‚Äù

---

## **5Ô∏è‚É£ SFT & RLHF (Why ChatGPT behaves nicely)**

### **Supervised Fine-Tuning (SFT)**

- Humans provide high-quality responses.
    
- Model learns: ‚ÄúGiven this input, this is the right answer.‚Äù
    

### **RLHF (Reinforcement Learning from Human Feedback)**

Flow:

```
Base Model
 ‚Üí SFT
 ‚Üí Generate responses
 ‚Üí Humans rank responses
 ‚Üí Train reward model
 ‚Üí Optimize model with RL
```

**What RLHF does:**

- Reduces hallucinations
    
- Improves safety, helpfulness, tone
    
- Aligns model with human expectations
    

**Interview Line:**

> ‚ÄúRLHF doesn‚Äôt make the model smarter ‚Äî it makes it more aligned with human preferences.‚Äù

---

## **6Ô∏è‚É£ Tokenization Tooling**

- **tiktoken**: OpenAI‚Äôs high-performance tokenizer.
    
- Token vocabularies differ per model ‚Üí affects cost & context length.
    

---

## **7Ô∏è‚É£ Failure & Trade-Off Points**

- Greedy decoding ‚Üí dull responses.
    
- High temperature ‚Üí hallucinations.
    
- RLHF can reduce raw capability slightly but improves safety.
    
- Tokenization differences impact prompt design.
    

---

## **8Ô∏è‚É£ One-Minute Interview Explanation**

> ‚ÄúGPT models generate text autoregressively: tokenize ‚Üí compute logits ‚Üí convert to probabilities ‚Üí apply sampling ‚Üí pick next token ‚Üí repeat.  
> Attention allows each token to focus on relevant past tokens using QKV and causal masking.  
> After base training, SFT and RLHF shape the model‚Äôs behavior, improving helpfulness, safety, and alignment.‚Äù

---

If you‚Äôd like, next we‚Äôll do:

**Prompt Engineering + Tool Calling**  
which connects everything you‚Äôve learned so far into _real AI product architecture_.