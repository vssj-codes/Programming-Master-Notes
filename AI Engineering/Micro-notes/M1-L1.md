# Table-of-Contents

<!-- toc -->

- [**1️⃣ One-Line Core Idea**](#1%EF%B8%8F%E2%83%A3-one-line-core-idea)
- [**2️⃣ Context vs Prompt vs Fine-Tuning**](#2%EF%B8%8F%E2%83%A3-context-vs-prompt-vs-fine-tuning)
  * [**Context Engineering**](#context-engineering)
  * [**Prompt Engineering**](#prompt-engineering)
  * [**Fine-Tuning**](#fine-tuning)
- [**3️⃣ High-Level Flow: How ChatGPT Works**](#3%EF%B8%8F%E2%83%A3-high-level-flow-how-chatgpt-works)
- [**4️⃣ Tokenization & BPE**](#4%EF%B8%8F%E2%83%A3-tokenization--bpe)
  * [**Tokenization**](#tokenization)
  * [**Byte Pair Encoding (BPE)**](#byte-pair-encoding-bpe)
- [**5️⃣ Positional Embeddings**](#5%EF%B8%8F%E2%83%A3-positional-embeddings)
- [**6️⃣ Key Failure / Trade-Off Points**](#6%EF%B8%8F%E2%83%A3-key-failure--trade-off-points)
- [**7️⃣ One-Minute Interview Explanation**](#7%EF%B8%8F%E2%83%A3-one-minute-interview-explanation)

<!-- tocstop -->

---

## **1️⃣ One-Line Core Idea**

> _When you send a prompt, the LLM converts text → tokens → embeddings → runs them through transformer layers → predicts next tokens → converts them back into text._

---

## **2️⃣ Context vs Prompt vs Fine-Tuning**

### **Context Engineering**

- Supplying **external information** dynamically at runtime.
    
- Example: RAG, system messages, chat history.
    
- No model weights change.
    

### **Prompt Engineering**

- Structuring input instructions to steer behavior.
    
- Example: role prompting, chain-of-thought, formatting rules.
    

### **Fine-Tuning**

- Updating **model weights** with new training data.
    
- Used when behavior must be persistent and domain-specific.
    

**Interview Line:**

> “Prompt and context affect behavior at inference time, fine-tuning changes the model itself.”

---

## **3️⃣ High-Level Flow: How ChatGPT Works**

**Mental Diagram:**

```
User Text
  ↓
Tokenizer → Token IDs
  ↓
Embeddings + Positional Info
  ↓
Transformer Layers (Self-Attention + FFN)
  ↓
Logits (probability over vocabulary)
  ↓
Sampling Strategy
  ↓
Next Token → Loop
  ↓
Detokenization → Output Text
```

**Interview Line:**

> “LLMs don’t generate text directly, they generate tokens probabilistically one step at a time.”

---

## **4️⃣ Tokenization & BPE**

### **Tokenization**

- Splits text into **tokens**.
    
- Each token → unique **Token ID**.
    
- IDs index into the model’s embedding matrix.
    

### **Byte Pair Encoding (BPE)**

- Starts with bytes / characters.
    
- Merges frequent pairs into larger tokens.
    
- Optimizes vocabulary size & efficiency.
    

Example:  
`p + izza → pizza`

**Why it matters:**

- Controls model speed, cost, and how text is represented.
    

**Interview Line:**

> “BPE allows the model to handle rare words while keeping the vocabulary compact.”

---

## **5️⃣ Positional Embeddings**

Transformers are **order-agnostic** → need position info.

Final embedding for each token:

```
FinalEmbedding = TokenEmbedding + PositionalEmbedding
```

This allows the model to know:

> “This token came before that token.”

**Interview Line:**

> “Without positional embeddings, the model wouldn’t understand sequence order.”

---

## **6️⃣ Key Failure / Trade-Off Points**

- Tokenization can distort meaning (same word → different tokens).
    
- Long prompts → context window limits.
    
- BPE favors frequent patterns → rare domain terms may fragment.
    

**Interview Insight:**

> “Many hallucination issues start at tokenization and context management, not the model itself.”

---

## **7️⃣ One-Minute Interview Explanation**

> “When I send a prompt, it’s first tokenized using BPE into token IDs.  
> Each token ID is converted into embeddings and combined with positional embeddings.  
> These embeddings flow through transformer layers with self-attention, producing logits over the vocabulary.  
> A sampling strategy picks the next token, and this repeats until the final response is generated and detokenized into text.”
