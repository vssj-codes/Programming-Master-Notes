# Table-of-Contents

<!-- toc -->

- [Best practices](#best-practices)
        * [**Github repo (for code):** [**https://github.com/vv176/SimpleExamples**](https://github.com/vv176/SimpleExamples)](#github-repo-for-code-httpsgithubcomvv176simpleexampleshttpsgithubcomvv176simpleexamples)
- [**Topic 1: Prompt Engineering â€” What, Why & How**](#topic-1-prompt-engineering--what-why--how)
      - [**ğŸ” What is Prompt Engineering?**](#%F0%9F%94%8D-what-is-prompt-engineering)
        * [Prompt Engineering is the art and science of **adapting the behavior of a large language model (LLM)**](#prompt-engineering-is-the-art-and-science-of-adapting-the-behavior-of-a-large-language-model-llm)
        * [to perform a desired task â€” without changing its internal weights, architecture, or doing any fine-tuning.](#to-perform-a-desired-task--without-changing-its-internal-weights-architecture-or-doing-any-fine-tuning)
        * [Itâ€™s the process of shaping **what the model does**, just by tweaking what you say to it (i.e., the prompt).](#its-the-process-of-shaping-what-the-model-does-just-by-tweaking-what-you-say-to-it-ie-the-prompt)
      - [**ğŸ’¡ Why Prompt Engineering?**](#%F0%9F%92%A1-why-prompt-engineering)
        * [Vivek called it the **"most underrated yet crucial part"** of AI engineering, and made one thing very clear:](#vivek-called-it-the-most-underrated-yet-crucial-part-of-ai-engineering-and-made-one-thing-very-clear)
        * [â In production-grade AI applications, **90% of the grunt work** is prompt engineering. â](#%E2%9D%9D-in-production-grade-ai-applications-90%25-of-the-grunt-work-is-prompt-engineering-%E2%9D%9E)
        * [It's not just about writing clever instructions â€” it's about **making LLMs behave the way your product needs**, under unpredictable, noisy, and diverse conditions.](#its-not-just-about-writing-clever-instructions--its-about-making-llms-behave-the-way-your-product-needs-under-unpredictable-noisy-and-diverse-conditions)
      - [**ğŸ›  Analogy: ML Models Before Prompting Era**](#%F0%9F%9B%A0-analogy-ml-models-before-prompting-era)
        * [In the traditional ML world:](#in-the-traditional-ml-world)
        * [This meant _huge development overhead_ for every new task.](#this-meant-_huge-development-overhead_-for-every-new-task)
      - [**âš™ What Prompt Engineering Unlocks**](#%E2%9A%99-what-prompt-engineering-unlocks)
        * [â Just supply the right prompt, and the behavior adapts. Thatâ€™s your superpower. â](#%E2%9D%9D-just-supply-the-right-prompt-and-the-behavior-adapts-thats-your-superpower-%E2%9D%9E)
      - [**ğŸ§¬ Key Insight: Prompt Engineering = Behavior Hacking Without Code**](#%F0%9F%A7%AC-key-insight-prompt-engineering--behavior-hacking-without-code)
        * [Prompt engineering is about:](#prompt-engineering-is-about)
        * [Youâ€™re not editing the modelâ€™s neurons â€” you're manipulating its perception.](#youre-not-editing-the-models-neurons--youre-manipulating-its-perception)
        * [Itâ€™s like giving the same person different instructions and getting them to write poems, solve math, or](#its-like-giving-the-same-person-different-instructions-and-getting-them-to-write-poems-solve-math-or)
        * [summarize policy â€” **without ever changing who they are.**](#summarize-policy--without-ever-changing-who-they-are)
      - [**TL; DR;**](#tl-dr)
- [**Topic 2: The Myth of Prompt Engineering**](#topic-2-the-myth-of-prompt-engineering)
    + [**âŒ Myth #1: â€œPrompting is Easyâ€**](#%E2%9D%8C-myth-%231-prompting-is-easy)
      - [Many believe:](#many-believe)
      - [âœ… But in reality:](#%E2%9C%85-but-in-reality)
        * [**Prompting is hard-core engineering** â€” often messier, more unpredictable, and more trial-and-error than writing traditional software.](#prompting-is-hard-core-engineering--often-messier-more-unpredictable-and-more-trial-and-error-than-writing-traditional-software)
    + [**âŒ Myth #2: â€œItâ€™s Not Engineering at Allâ€**](#%E2%9D%8C-myth-%232-its-not-engineering-at-all)
        * [People question the term â€œengineeringâ€ in prompt engineering.](#people-question-the-term-engineering-in-prompt-engineering)
        * [Vivek addressed this directly:](#vivek-addressed-this-directly)
        * [â Prompt engineering requires more engineering than deterministic code. Youâ€™ll understand why when you try to test and debug LLM behavior. â](#%E2%9D%9D-prompt-engineering-requires-more-engineering-than-deterministic-code-youll-understand-why-when-you-try-to-test-and-debug-llm-behavior-%E2%9D%9E)
    + [**âš™ So, Why Is It Actually Engineering?**](#%E2%9A%99-so-why-is-it-actually-engineering)
        * [Because it shares the same **rigor, iteration, and debugging** mindset as software engineering â€” but applied to a **non-deterministic black box.**](#because-it-shares-the-same-rigor-iteration-and-debugging-mindset-as-software-engineering--but-applied-to-a-non-deterministic-black-box)
        * [You still need to:](#you-still-need-to)
        * [The only difference? Your code is not **executable logic**, it's **natural language** with embedded logic.](#the-only-difference-your-code-is-not-executable-logic-its-natural-language-with-embedded-logic)
    + [**ğŸ§ª Software Engineering vs. Prompt Engineering: A Testing Analogy**](#%F0%9F%A7%AA-software-engineering-vs-prompt-engineering-a-testing-analogy)
      - [**âœ… In Traditional Software (Deterministic)**](#%E2%9C%85-in-traditional-software-deterministic)
        * [if user_id is None:](#if-user_id-is-none)
        * [raise NullPointerException](#raise-nullpointerexception)
        * [â†’ Always fails for None. Always fixable with one if-check.](#%E2%86%92-always-fails-for-none-always-fixable-with-one-if-check)
      - [**âŒ In Prompt Engineering (Non-Deterministic)**](#%E2%9D%8C-in-prompt-engineering-non-deterministic)
        * [â Same prompt, same input â†’ 3 different behaviors. â](#%E2%9D%9D-same-prompt-same-input-%E2%86%92-3-different-behaviors-%E2%9D%9E)
        * [This means:](#this-means)
        * [ğŸ” And when you fix one issue, others might get worse. Thatâ€™s the ripple effect (a.k.a. side effects).](#%F0%9F%94%81-and-when-you-fix-one-issue-others-might-get-worse-thats-the-ripple-effect-aka-side-effects)
      - [**ğŸ§  Real-Life Analogy: Doctor Diagnosing Side Effects**](#%F0%9F%A7%A0-real-life-analogy-doctor-diagnosing-side-effects)
        * [Prompt engineering is like a doctor prescribing medicine:](#prompt-engineering-is-like-a-doctor-prescribing-medicine)
        * [You need to:](#you-need-to)
      - [**ğŸ§  Black Box & Non-Determinism: Root Cause**](#%F0%9F%A7%A0-black-box--non-determinism-root-cause)
        * [Two properties make LLM prompting inherently hard:](#two-properties-make-llm-prompting-inherently-hard)
        * [So you canâ€™t treat LLMs like APIs with clear contracts.](#so-you-cant-treat-llms-like-apis-with-clear-contracts)
      - [**âš¡ Key Takeaway**](#%E2%9A%A1-key-takeaway)
        * [â If software engineering is deterministic Lego-building, prompt engineering is **behavioral psychology** with an unpredictable alien. â](#%E2%9D%9D-if-software-engineering-is-deterministic-lego-building-prompt-engineering-is-behavioral-psychology-with-an-unpredictable-alien-%E2%9D%9E)
        * [Itâ€™s not about language. Itâ€™s about **behavior design** under uncertainty.](#its-not-about-language-its-about-behavior-design-under-uncertainty)
      - [**TL; DR;**](#tl-dr-1)
- [**Topic 3: Prompt Template â€” How to Write a V0 Prompt**](#topic-3-prompt-template--how-to-write-a-v0-prompt)
        * [**This is the foundational structure you can use as your first commit â€” your Version 0 â€” when building LLM-based apps.**](#this-is-the-foundational-structure-you-can-use-as-your-first-commit--your-version-0--when-building-llm-based-apps)
    + [**ğŸ§± V0 Prompt Structure (3-Part Template)**](#%F0%9F%A7%B1-v0-prompt-structure-3-part-template)
      - [**1. ğŸ§  Task Description (System Prompt Style)**](#1-%F0%9F%A7%A0-task-description-system-prompt-style)
        * [Tell the model **what it needs to do, who it should be,** and **how to respond.**](#tell-the-model-what-it-needs-to-do-who-it-should-be-and-how-to-respond)
        * [This part includes:](#this-part-includes)
        * [ğŸ§  **Think of this like a system prompt** â€” youâ€™re configuring the model's behavioral blueprint here.](#%F0%9F%A7%A0-think-of-this-like-a-system-prompt--youre-configuring-the-models-behavioral-blueprint-here)
    + [**2. ğŸ” Examples (Few-shot / Explanation-based)**](#2-%F0%9F%94%81-examples-few-shot--explanation-based)
        * [Provide examples of how to do the task.](#provide-examples-of-how-to-do-the-task)
        * [These can include:](#these-can-include)
      - [**ğŸ§  Vivek's insight:**](#%F0%9F%A7%A0-viveks-insight)
        * [â Examples are not just I/O â€” theyâ€™re a way to shape the modelâ€™s thinking process. â](#%E2%9D%9D-examples-are-not-just-io--theyre-a-way-to-shape-the-models-thinking-process-%E2%9D%9E)
      - [**ğŸ’¬ Example Case (Movie Recommender Prompt from class Q&A):**](#%F0%9F%92%AC-example-case-movie-recommender-prompt-from-class-qa)
        * [If user input is:](#if-user-input-is)
        * [â€œMovies about prison lifeâ€](#movies-about-prison-life)
        * [You can provide example output:](#you-can-provide-example-output)
        * [Shawshank Redemption, because itâ€™s widely recognized, fits the theme, and has a great IMDb score.](#shawshank-redemption-because-its-widely-recognized-fits-the-theme-and-has-a-great-imdb-score)
      - [**ğŸ§  Why it matters:**](#%F0%9F%A7%A0-why-it-matters)
        * [Without this, LLM might return Arabic or Korean prison movies â€” **not aligned with your userâ€™s context.**](#without-this-llm-might-return-arabic-or-korean-prison-movies--not-aligned-with-your-users-context)
    + [**3. ğŸ§¾ The Task (Actual User Input)**](#3-%F0%9F%A7%BE-the-task-actual-user-input)
        * [This is **the real user query** or input data that needs to be processed now.](#this-is-the-real-user-query-or-input-data-that-needs-to-be-processed-now)
      - [**ğŸ§ª Class Example (Stand-Up Summary Task):**](#%F0%9F%A7%AA-class-example-stand-up-summary-task)
        * [**Practice Problem:**](#practice-problem)
        * [=============](#)
        * [You have transcript of morning standup that happened in team.](#you-have-transcript-of-morning-standup-that-happened-in-team)
        * [You want to take that transcript and convert it into a short summary also action items(â€œtaskâ€, â€œassigneeâ€, â€œdue dateâ€)](#you-want-to-take-that-transcript-and-convert-it-into-a-short-summary-also-action-itemstask-assignee-due-date)
        * [V0 Prompt](#v0-prompt)
        * [========](#)
        * [**Task description:**](#task-description)
      - [**Rules:**](#rules)
      - [**Example(s) of how to do this task**](#examples-of-how-to-do-this-task)
    + [**The task:**](#the-task)
    + [**TL;DR:** 3-Part V0 Prompt Template](#tldr-3-part-v0-prompt-template)
      - [**ğŸ”§ Pro Tip:**](#%F0%9F%94%A7-pro-tip)
        * [Start with this format. Once you hit edge cases or regressions (e.g. LLM skips steps, mislabels fields), go back and:](#start-with-this-format-once-you-hit-edge-cases-or-regressions-eg-llm-skips-steps-mislabels-fields-go-back-and)
        * [This is how prompt engineering **iterates like product engineering.**](#this-is-how-prompt-engineering-iterates-like-product-engineering)
- [**Topic 4: Rules of Prompting + Role of Evals**](#topic-4-rules-of-prompting--role-of-evals)
      - [**ğŸ¯ 1. Be Explicit**](#%F0%9F%8E%AF-1-be-explicit)
        * [Vivek repeatedly emphasized:](#vivek-repeatedly-emphasized)
        * [â If your prompt is vague, the LLM will hallucinate. Be absurdly specific. â](#%E2%9D%9D-if-your-prompt-is-vague-the-llm-will-hallucinate-be-absurdly-specific-%E2%9D%9E)
      - [**âœ… Real Example from Class:**](#%E2%9C%85-real-example-from-class)
        * [In his homework extraction use-case (images from NCERT textbook):](#in-his-homework-extraction-use-case-images-from-ncert-textbook)
        * [â†’ â€œExtract all problems from these pages.â€](#%E2%86%92-extract-all-problems-from-these-pages)
        * [âš  Result: The LLM hallucinated questions from random paragraphs.](#%E2%9A%A0-result-the-llm-hallucinated-questions-from-random-paragraphs)
        * [âœ… Fix: He rewrote the prompt to say:](#%E2%9C%85-fix-he-rewrote-the-prompt-to-say)
        * [â€œOnly extract solved examples and exercise questions. Solved examples have a labeled solution. Skip all concept-explaining paragraphs, even if they contain a question mark.â€](#only-extract-solved-examples-and-exercise-questions-solved-examples-have-a-labeled-solution-skip-all-concept-explaining-paragraphs-even-if-they-contain-a-question-mark)
        * [ğŸ“Œ **Lesson: Explicit rules reduce hallucination**, especially in noisy, unstructured inputs.](#%F0%9F%93%8C-lesson-explicit-rules-reduce-hallucination-especially-in-noisy-unstructured-inputs)
    + [**ğŸ‘¤ 2. Use a Persona / Role**](#%F0%9F%91%A4-2-use-a-persona--role)
        * [Tell the model who it's supposed to be.](#tell-the-model-who-its-supposed-to-be)
        * [Common pattern:](#common-pattern)
        * [â€œYou are a domain expert / experienced engineer / school teacher / recruiterâ€¦â€](#you-are-a-domain-expert--experienced-engineer--school-teacher--recruiter)
        * [ğŸ§  Why? It primes the modelâ€™s tone, domain knowledge, and response boundaries.](#%F0%9F%A7%A0-why-it-primes-the-models-tone-domain-knowledge-and-response-boundaries)
        * [Even though Vivek mentioned that engineers at Anthropic donâ€™t always favor this, he acknowledged:](#even-though-vivek-mentioned-that-engineers-at-anthropic-dont-always-favor-this-he-acknowledged)
        * [â It works in most real-world prompts. Thatâ€™s why itâ€™s used â€” not because itâ€™s a guarantee, but because it helps.â](#%E2%9D%9D-it-works-in-most-real-world-prompts-thats-why-its-used--not-because-its-a-guarantee-but-because-it-helps%E2%9D%9E)
    + [**ğŸ§¾ 3. Specify Output Format**](#%F0%9F%A7%BE-3-specify-output-format)
        * [Never let LLMs respond freely when structure matters.](#never-let-llms-respond-freely-when-structure-matters)
        * [**Example from Class (Standup Transcript Task):**](#example-from-class-standup-transcript-task)
        * [Output must include:](#output-must-include)
        * [{ "task": "Add load test to CI", "assignee": "Carol", "due_date": "Wednesday" }](#-task-add-load-test-to-ci-assignee-carol-due_date-wednesday-)
    + [**ğŸ§ª 4. Add & Analyze Edge Cases**](#%F0%9F%A7%AA-4-add--analyze-edge-cases)
        * [Vivek shared multiple examples where **new errors** forced him to **add edge-case rules** in the prompt:](#vivek-shared-multiple-examples-where-new-errors-forced-him-to-add-edge-case-rules-in-the-prompt)
- [**ğŸ“š 5. In-Context Learning (ICL)**](#%F0%9F%93%9A-5-in-context-learning-icl)
      - [**ğŸ“– Chapter 10 Example (Homework Extraction):**](#%F0%9F%93%96-chapter-10-example-homework-extraction)
      - [**âœ… Rule in Prompt:**](#%E2%9C%85-rule-in-prompt)
      - [**ğŸ“Š Role of Evals in Prompt Engineering**](#%F0%9F%93%8A-role-of-evals-in-prompt-engineering)
      - [**ğŸ§  Analogy: Medical Testing**](#%F0%9F%A7%A0-analogy-medical-testing)
      - [**How evals work:**](#how-evals-work)
      - [**âœ… TL;DR Summary**](#%E2%9C%85-tldr-summary)

<!-- tocstop -->

---

## Best practices

###### **Github repo (for code):**Â [**https://github.com/vv176/SimpleExamples**](https://github.com/vv176/SimpleExamples)Â 


## **Topic 1: Prompt Engineering â€” What, Why & How**


##### **ğŸ” What is Prompt Engineering?**

###### Prompt Engineering is the art and science ofÂ **adapting the behavior of a large language model (LLM)**

###### to perform a desired task â€” without changing its internal weights, architecture, or doing any fine-tuning.

###### Itâ€™s the process of shapingÂ **what the model does**, just by tweaking what you say to it (i.e., the prompt).

  

##### **ğŸ’¡ Why Prompt Engineering?**

###### Vivek called it theÂ **"most underrated yet crucial part"**Â of AI engineering, and made one thing very clear:

###### â In production-grade AI applications,Â **90% of the grunt work**Â is prompt engineering. â

###### It's not just about writing clever instructions â€” it's aboutÂ **making LLMs behave the way your product needs**, under unpredictable, noisy, and diverse conditions.

  

##### **ğŸ›  Analogy: ML Models Before Prompting Era**

###### In the traditional ML world:

- You had a trained model that could doÂ **Task A**Â â€” say, text summarization.
- Now you want to doÂ **Task B**Â â€” spam classification.
- You had no choice but to:
    - **Retrain the model**
    - **Rewire the architecture**
    - **Adjust the weights**

###### This meantÂ _huge development overhead_Â for every new task.Â 

  

##### **âš™ What Prompt Engineering Unlocks**

- With LLMs + prompts, you can:
- Use theÂ **same base model**
- Without retraining or modifying its internals
    - And get it to do:
    - Summarization âœ…
    - Classification âœ…
    - Code generation âœ…
    - Translation âœ…
    - Anything else âœ…

###### â Just supply the right prompt, and the behavior adapts. Thatâ€™s your superpower. âÂ 

  

##### **ğŸ§¬ Key Insight: Prompt Engineering = Behavior Hacking Without Code**Â 

###### Prompt engineering is about:Â 

â€œAdapting behavior of a system you donâ€™t control, by designing the right input signal.â€

###### Youâ€™re not editing the modelâ€™s neurons â€” you're manipulating its perception.

###### Itâ€™s like giving the same person different instructions and getting them to write poems, solve math, or

###### summarize policy â€”Â **without ever changing who they are.**Â 

  

##### **TL; DR;**

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756889583/Screenshot_2025-09-03_at_2.05.22_PM_oruzvf.png)

  

## **Topic 2: The Myth of Prompt Engineering**Â 

######   

#### **âŒ Myth #1: â€œPrompting is Easyâ€**

##### Many believe:

- "Prompt engineering is just good English.â€
- â€œAnyone can do it with basic language skills.â€
- â€œWhy even call it engineering?â€

##### âœ… But in reality:

###### **Prompting is hard-core engineering**Â â€” often messier, more unpredictable, and more trial-and-error than writing traditional software.

  

#### **âŒ Myth #2: â€œItâ€™s Not Engineering at Allâ€**

###### People question the term â€œengineeringâ€ in prompt engineering.

###### Vivek addressed this directly:

###### â Prompt engineering requires more engineering than deterministic code. Youâ€™ll understand why when you try to test and debug LLM behavior. â

######   

#### **âš™ So, Why Is It Actually Engineering?**

###### Because it shares the sameÂ **rigor, iteration, and debugging**Â mindset as software engineering â€” but applied to aÂ **non-deterministic black box.**

###### You still need to:

- Analyze failures
- Refactor inputsÂ 
- Design for edge cases
- Monitor performance
- Build guardrails

###### The only difference? Your code is notÂ **executable logic**, it'sÂ **natural language**Â with embedded logic.

######   

#### **ğŸ§ª Software Engineering vs. Prompt Engineering: A Testing Analogy**

#####   

##### **âœ… In Traditional Software (Deterministic)**

- Code behaves the same way every time for a given input
- If it fails once, itâ€™ll fail every time for the same condition
- Example:Â 

###### Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if user_id is None:

###### Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â raise NullPointerException

###### Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â†’ Always fails for None. Always fixable with one if-check.

- Testing =Â **unit tests, integration tests, regression tests**
- Easy to isolate root causes and patch them

######   

##### **âŒ In Prompt Engineering (Non-Deterministic)**

###### â Same prompt, same input â†’ 3 different behaviors. â

- First run: excellent response
- Second run: slightly worse
- Third run: totally off
- EvenÂ **zero-shot prompts**Â can perform inconsistently

###### This means:

- YouÂ **canâ€™t write 10 unit tests**Â and call it done
- You mustÂ **analyze 1000+ prompt invocations**
- ThenÂ **categorize failures**Â (e.g. 40% = hallucination, 30% = format errors, etc.)

###### ğŸ” And when you fix one issue, others might get worse. Thatâ€™s the ripple effect (a.k.a. side effects).

  

##### **ğŸ§  Real-Life Analogy: Doctor Diagnosing Side Effects**

###### Prompt engineering is like a doctor prescribing medicine:

- You fix one symptom (e.g. hallucination)
- But your fix creates new issues (e.g. skips some valid answers)

###### You need to:

- Run evals = diagnostic tests
- Track what gets better vs what regresses
- Iterate againÂ 

######   

##### **ğŸ§  Black Box & Non-Determinism: Root Cause**

###### Two properties make LLM prompting inherently hard:Â 

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756890472/Screenshot_2025-09-03_at_2.37.03_PM_ryt8fu.png)

###### So you canâ€™t treat LLMs like APIs with clear contracts.Â 

  

##### **âš¡ Key Takeaway**

###### â If software engineering is deterministic Lego-building, prompt engineering isÂ **behavioral psychology**Â with an unpredictable alien. â

###### Itâ€™s not about language. Itâ€™s aboutÂ **behavior design**Â under uncertainty.Â 

  

##### **TL; DR;**

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756890614/Screenshot_2025-09-03_at_2.39.56_PM_tvcrqx.png)

  

## **Topic 3: Prompt Template â€” How to Write a V0 Prompt**

###### **This is the foundational structure you can use as your first commit â€” your Version 0 â€” when building LLM-based apps.**

######   

#### **ğŸ§± V0 Prompt Structure (3-Part Template)**

######   

##### **1. ğŸ§  Task Description (System Prompt Style)**

###### Tell the modelÂ **what it needs to do, who it should be,**Â andÂ **how to respond.**

###### This part includes:Â 

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756890811/Screenshot_2025-09-03_at_2.43.18_PM_lwmy1y.png)

  

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756890849/Screenshot_2025-09-03_at_2.43.53_PM_jpehcf.png)

###### ğŸ§ Â **Think of this like a system prompt**Â â€” youâ€™re configuring the model's behavioral blueprint here.

######   

#### **2. ğŸ” Examples (Few-shot / Explanation-based)**

###### Provide examples of how to do the task.

###### These can include:

- Input-output pairs
- Annotated examples (explain why this output is correct)
- Format styles
- Doâ€™s and Don'ts

##### **ğŸ§  Vivek's insight:**

###### â Examples are not just I/O â€” theyâ€™re a way to shape the modelâ€™s thinking process. â

##### **ğŸ’¬ Example Case (Movie Recommender Prompt from class Q&A):**

###### If user input is:

###### â€œMovies about prison lifeâ€

###### You can provide example output:

###### Shawshank Redemption, because itâ€™s widely recognized, fits the theme, and has a great IMDb score.

  

##### **ğŸ§  Why it matters:**

###### Without this, LLM might return Arabic or Korean prison movies â€”Â **not aligned with your userâ€™s context.**

######   

#### **3. ğŸ§¾ The Task (Actual User Input)**

###### This isÂ **the real user query**Â or input data that needs to be processed now.

- After setting up the behavior (task + examples), this is where the actual question or data is injected.
- The better your first two sections, the more accurate and aligned the LLMâ€™s response will be here.

##### **ğŸ§ª Class Example (Stand-Up Summary Task):**

###### **Practice Problem:**

###### =============

###### You have transcript of morning standup that happened in team.

###### You want to take that transcript and convert it into a short summary also action items(â€œtaskâ€, â€œassigneeâ€, â€œdue dateâ€)

###### V0 Prompt

###### ========

###### **Task description:**

You are a note-taker for engineering standups.

Summarize the meeting for a PM audience in â‰¤120 words and extract action items (task, assignee, due_date if mentioned).

Output only valid JSON:Â 

{

Â Â Â Â Â Â "summary": "â‰¤120 words, PM-friendly",

Â Â Â Â Â Â Â "actions": [ {

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "task": "string",

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "assignee": "string|null",

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "due_date": "YYYY-MM-DD|null"

Â Â Â Â Â Â Â Â }]

}Â 

######   

##### **Rules:**

If no assignee/due date is stated, set field to null

  

##### **Example(s) of how to do this task**

Sample Input :

Alice: Search latency spiked to 420ms after Friday's deployment.

Bob: Root cause is the new logging middleware; reverting cuts it to ~180ms.

Carol: I'll add a load test to CI by Wed.

Bob: Cool, I will go through the code of middleware.

  

  

Sample Output :

{

Â Â Â Â Â Â Â Â Â "summary": "Latency rose to 420ms post-deploy; reverting logging middleware drops it to ~180ms. Team will add a CI load test and read middlewareâ€™s code.",

Â Â Â Â Â Â Â Â Â Â "actions": [

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â {

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "task": "Add load test to CI",

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "assignee": "Carol",

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "due_date": "null"

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â },

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â {Â 

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "task": "Code study of middleware",

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "assignee": "Bob",

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "due_date": "null"

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â }

Â Â Â Â Â Â Â Â Â Â ]

}

  

#### **The task:**

Summarize and extract actions from the transcript below. Respond with JSON only.

Priya: Weâ€™ve encountered a NullPointerException in the order-service module after the latest deploy.

Arjun: Thatâ€™s likely on meâ€”my recent code shipped and I suspect I missed a null check in the

OrderValidator. Iâ€™ll dig into the stack trace now.

Riya: Iâ€™m on-call; Iâ€™ll create a hotfix and ship it behind a flag.

Karan: Greatâ€”Riya, ping me once your code is ready. Iâ€™ll review the PR immediately.Â 

  

#### **TL;DR:**Â 3-Part V0 Prompt TemplateÂ 

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756891720/Screenshot_2025-09-03_at_2.55.54_PM_flyuou.png)

  

##### **ğŸ”§ Pro Tip:**

###### Start with this format. Once you hit edge cases or regressions (e.g. LLM skips steps, mislabels fields), go back and:Â 

- Add rules
- Refactor examples
- Split prompt into smaller parts if needed

###### This is how prompt engineeringÂ **iterates like product engineering.**Â 

######   

## **Topic 4: Rules of Prompting + Role of Evals**

##### **ğŸ¯ 1. Be Explicit**

###### Vivek repeatedly emphasized:

###### â If your prompt is vague, the LLM will hallucinate. Be absurdly specific. â

######   

##### **âœ… Real Example from Class:**

###### In his homework extraction use-case (images from NCERT textbook):

- Initially, the prompt said:

###### Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â†’ â€œExtract all problems from these pages.â€

  

###### âš  Result: The LLM hallucinated questions from random paragraphs.

###### âœ… Fix: He rewrote the prompt to say:

###### â€œOnly extract solved examples and exercise questions. Solved examples have a labeled solution. Skip all concept-explaining paragraphs, even if they contain a question mark.â€

###### ğŸ“ŒÂ **Lesson: Explicit rules reduce hallucination**, especially in noisy, unstructured inputs.Â 

######   

#### **ğŸ‘¤ 2. Use a Persona / Role**

###### Tell the model who it's supposed to be.

###### Common pattern:

###### â€œYou are a domain expert / experienced engineer / school teacher / recruiterâ€¦â€

###### ğŸ§  Why? It primes the modelâ€™s tone, domain knowledge, and response boundaries.

###### Even though Vivek mentioned that engineers at Anthropic donâ€™t always favor this, he acknowledged:

###### â It works in most real-world prompts. Thatâ€™s why itâ€™s used â€” not because itâ€™s a guarantee, but because it helps.â

######   

#### **ğŸ§¾ 3. Specify Output Format**

###### Never let LLMs respond freely when structure matters.

- In free-text mode: model can be verbose, inconsistent, or unordered
- With a defined output schema (JSON, Markdown, numbered list), you can:
    - Parse the output
    - Compare against expectations
    - Integrate into systems

  

###### **Example from Class (Standup Transcript Task):**

###### Output must include:Â 

- A summary
- A structured list of action items:

###### Â Â Â Â Â Â Â Â Â Â { "task": "Add load test to CI", "assignee": "Carol", "due_date": "Wednesday" }

  

#### **ğŸ§ª 4. Add & Analyze Edge Cases**

###### Vivek shared multiple examples whereÂ **new errors**Â forced him toÂ **add edge-case rules**Â in the prompt:Â 

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756892228/Screenshot_2025-09-03_at_3.06.53_PM_hojgkt.png)

  

## **ğŸ“š 5. In-Context Learning (ICL)**

Teach the model how to solve a task by showing examples within the prompt.

##### **ğŸ“– Chapter 10 Example (Homework Extraction):**

LLM had to extract questions from textbook pages â€” but not all were straightforward.

Problem:

- Sometimes the LLM found questions with no solution
- Other times, the solution was split across pages
- Sometimes, it hallucinated from examples

Fix:

- Vivek explicitly said:

Â Â Â Â Â Â Â Â Â Â Â â€œA solved example is only valid if it contains both the problem and the solution. If either is missing,Â **say so.**Â â€

  

##### **âœ… Rule in Prompt:**

- If there is:
    - âœ… Evidence â†’ extract problem
    - âŒ Missing info â†’ report it
    - ğŸ•µ No evidence â†’ skip the image

This isÂ _in-context teaching_Â the LLM what grounding means.Â 

  

##### **ğŸ“Š Role of Evals in Prompt Engineering**Â 

**Evals = Unit Tests for Prompts**

Used to:

- Catch regressions
- Measure performance across inputs
- Quantify side effects

##### **ğŸ§  Analogy: Medical Testing**

Vivekâ€™s metaphor:

â€œImagine a doctor giving you medicine for leg pain. After 7 days, the leg is fine, but your liver enzymes are off. Thatâ€™s what evals show â€” the unexpected side effects of your prompt change.â€

  

##### **How evals work:**

- You define:
    - âœ… Inputs (e.g. folder of 16 image sets)
    - âœ… Expected outputs
    - âœ… Actual outputs from the LLM
- Run an eval script to:
    - Compare actual vs expected
    - Categorize failure cases (e.g. 0.5% hallucination, 10% missing info)

ğŸ§  You can even use LLM asÂ **a judge**Â in the eval pipeline (to compare semantic similarity).

  

##### **âœ… TL;DR Summary**Â 

![](https://res.cloudinary.com/dfgtstjy6/image/upload/v1756892912/Screenshot_2025-09-03_at_3.16.59_PM_rpb5vg.png)